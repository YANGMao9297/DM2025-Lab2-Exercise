{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Information**\n",
    "Name: 楊宏文\n",
    "\n",
    "Student ID: NCKU_E84116277\n",
    "\n",
    "GitHub ID: YANGMao9297\n",
    "\n",
    "Kaggle name: Peter_YANG9297\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "![pic_ranking.png](./pics/pic_ranking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
    "\n",
    "**Environment recommendations to solve lab 2:**\n",
    "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
    "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
    "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
    "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models. \n",
    "\n",
    "## **Phase 1 (30 pts):**\n",
    "\n",
    "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "## **Phase 2 (30 pts):**\n",
    "\n",
    "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "## **Phase 3 (40 pts):**\n",
    "\n",
    "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/t/3a2df4c6d6b4417e8bf718ed648d7554)**. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements: \n",
    "* Your preprocessing steps.\n",
    "* The feature engineering steps.\n",
    "* Explanation of your model.\n",
    "\n",
    "* **`Bonus (5 pts):`**\n",
    "    * You will have to describe more detail in the previous steps.\n",
    "    * Mention different things you tried.\n",
    "    * Mention insights you gained. \n",
    "\n",
    "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "**`Things to note for Phase 3:`**\n",
    "\n",
    "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
    "\n",
    "* **Push the code used for the competition to your repository**.\n",
    "\n",
    "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
    "\n",
    "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
    "\n",
    "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
    "\n",
    "## **Deadlines:**\n",
    "\n",
    "![lab2_deadlines](./pics/lab2_deadlines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next you will find the template report with some simple markdown syntax explanations, use it to structure your content.\n",
    "\n",
    "You can delete the syntax suggestions after you use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# **Project Report**\n",
    "\n",
    "**Syntax:** `#` creates the largest heading (H1).\n",
    "\n",
    "---\n",
    "**Syntax:** `---` creates a horizontal rule (a separator line).\n",
    "\n",
    "## 1. Model Development (10 pts Required)\n",
    "\n",
    "**Syntax:** `##` creates a secondary heading (H2).\n",
    "\n",
    "**Describe briefly each section, you can add graphs/charts to support your explanations.**\n",
    "\n",
    "### 1.1 Preprocessing Steps\n",
    "\n",
    "**Syntax:** `###` creates a tertiary heading (H3).\n",
    "\n",
    "[Content for Preprocessing]\n",
    "\n",
    "**Example Syntax for Content:**\n",
    "*   **Bold text:** `**text**`\n",
    "*   *Italic text*: `*text*`\n",
    "*   Bullet point list:\n",
    "    * Item 1\n",
    "    * Item 2\n",
    "\n",
    "Markdown Syntax to Add Image: `![Description of the Image](./your_local_folder/name_of_the_image.png)`\n",
    "\n",
    "![Example Markdown Syntax to Add Image](./pics/example_md_img.png)\n",
    "\n",
    "### 1.2 Feature Engineering Steps\n",
    "\n",
    "[Content for Feature Engineering]\n",
    "\n",
    "### 1.3 Explanation of Your Model\n",
    "\n",
    "[Content for Model Explanation]\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bonus Section (5 pts Optional)\n",
    "\n",
    "**Add more detail in previous sections**\n",
    "\n",
    "### 2.1 Mention Different Things You Tried\n",
    "\n",
    "[Content for Experiments]\n",
    "\n",
    "### 2.2 Mention Insights You Gained\n",
    "\n",
    "[Content for Insights]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "In this competition, we provide a text dataset from various sources. There are 6 classes (or say emotions) in our dataset: anger, disgust, fear, sadness, surprise, and joy.\n",
    "\n",
    "You have to clean the data by doing some pre-processing first. Then, apply feature engineering or any other data mining technique you have or haven't learned in the Data Mining course. The final goal is to learn a model that is able to predict the emotion behind each post.\n",
    "\n",
    "# Evaluation\n",
    "You will be evaluated by comparing your Submission CSV to the ground truth Solution CSV with respect the chosen the Mean F1 Score.\n",
    "\n",
    "This competition is worth 30% of your grade for the DM2025-Lab2-Homework. The scoring will be given according to your place in the Private Leaderboard ranking:\n",
    "\n",
    "Bottom 40%: Get 20% of the score (ie. 20% of 30% ) Top 41% - 100%: Get (101-x)% of the score, where x is your ranking in the leaderboard (ie. (101-x)% of 30% ) Submission Format Submit a CSV file with your predictions. It should contain two string type columns: id and emotion. Make sure you include the headers as well\n",
    "\n",
    "id, emotion 0x356d9a, surprise 0x2c7743, surprise 0x2c1eed, surprise 0x2826ea, surprise etc.\n",
    "\n",
    "# Dataset Description\n",
    "final_posts.json - the raw data in json (please check how to load and save json file to df)\n",
    "\n",
    "emotion.csv - Lists the emotion labels per post_id\n",
    "\n",
    "data_identification.csv - A file that identifies membership of training or testing set per post_id. Note that you won´t be provided with the labels for the testing set, but you will have to predict for these when you make your submission.\n",
    "\n",
    "sampleSubmission.csv - A submission format you should follow for submitting to the competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`From here on starts the code section for the competition.`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Competition Code**\n",
    "\n",
    "## 1. Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             id                                               text\n",
      "0      0x61fc95  We got the ranch, loaded our guns and sat up t...\n",
      "1      0x35663e  I bet there is an army of married couples who ...\n",
      "2      0xc78afe                         This could only end badly.\n",
      "3      0x90089c  My sister squeezed a lime in her milk when she...\n",
      "4      0xaba820         and that got my head bobbing a little bit.\n",
      "...         ...                                                ...\n",
      "64166  0x4afbe1  Guilty Gear actually did that before with Guil...\n",
      "64167  0xf5ba78                       One of my favorite episodes.\n",
      "64168  0x8f758e  I got my first raspberry from a crowd surfer f...\n",
      "64169  0xb5a35a  Texans and Astros both shut out tonight. Houst...\n",
      "64170  0x3a9174  Pre-prepare direction plays hale and hearty si...\n",
      "\n",
      "[64171 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 讀取原始 JSON 檔案並轉換為 DataFrame\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('dm-lab-2-private-competition/final_posts.json', 'r', encoding='utf-8') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "posts_data = []\n",
    "for item in raw_data:\n",
    "    post_info = item['root']['_source']['post']\n",
    "    posts_data.append({\n",
    "        'id': post_info['post_id'],\n",
    "        'text': post_info['text']\n",
    "    })\n",
    "\n",
    "df_posts = pd.DataFrame(posts_data)\n",
    "print(df_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練集大小: 47890\n",
      "測試集大小: 16281\n",
      "情緒類別: ['joy' 'fear' 'anger' 'surprise' 'sadness' 'disgust']\n"
     ]
    }
   ],
   "source": [
    "# 讀取訓練集的情緒標籤\n",
    "df_emotion = pd.read_csv('dm-lab-2-private-competition/emotion.csv')\n",
    "# 讀取訓練/測試集標示\n",
    "df_identification = pd.read_csv('dm-lab-2-private-competition/data_identification.csv')\n",
    "# 讀取提交格式範例\n",
    "df_sample = pd.read_csv('dm-lab-2-private-competition/sampleSubmission.csv')\n",
    "\n",
    "# 合併所有數據\n",
    "df = df_posts.merge(df_identification, on='id', how='left')\n",
    "df = df.merge(df_emotion, on='id', how='left')\n",
    "\n",
    "# 分割訓練集和測試集\n",
    "train_df = df[df['split'] == 'train'].copy()\n",
    "test_df = df[df['split'] == 'test'].copy()\n",
    "\n",
    "# 訓練集有標籤，測試集的標籤是 NaN\n",
    "print(f\"訓練集大小: {len(train_df)}\")\n",
    "print(f\"測試集大小: {len(test_df)}\")\n",
    "print(f\"情緒類別: {train_df['emotion'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ybcvx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ybcvx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ybcvx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ybcvx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "預處理前後對比:\n",
      "==================================================\n",
      "\n",
      "原始文本: I bet there is an army of married couples who did the same exact thing....\n",
      "清理後: bet army married couple exact thing...\n",
      "情緒標籤: joy\n",
      "--------------------------------------------------\n",
      "\n",
      "原始文本: This could only end badly....\n",
      "清理後: could end badly...\n",
      "情緒標籤: fear\n",
      "--------------------------------------------------\n",
      "\n",
      "原始文本: My sister squeezed a lime in her milk when she was 12. Same thing happened, but we told her it would...\n",
      "清理後: sister squeezed lime milk thing happened told would happen...\n",
      "情緒標籤: joy\n",
      "--------------------------------------------------\n",
      "\n",
      "訓練集清理後的空文本數量: 128\n",
      "測試集清理後的空文本數量: 33\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # 轉換為小寫\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 移除 URL\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # 移除 @用戶名 (mentions)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # 移除 #hashtag 符號但保留文字\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    \n",
    "    # 移除 HTML 標籤\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # 移除表情符號和特殊字符（保留基本標點）\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # 移除數字\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # 移除多餘空白\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # 分詞\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 移除停用詞並進行詞形還原\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens \n",
    "              if token not in stop_words and len(token) > 2]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "train_df['cleaned_text'] = train_df['text'].apply(preprocess_text)\n",
    "test_df['cleaned_text'] = test_df['text'].apply(preprocess_text)\n",
    "\n",
    "# 顯示預處理結果\n",
    "print(\"\\n預處理前後對比:\")\n",
    "print(\"=\"*50)\n",
    "for i in range(3):\n",
    "    print(f\"\\n原始文本: {train_df['text'].iloc[i][:100]}...\")\n",
    "    print(f\"清理後: {train_df['cleaned_text'].iloc[i][:100]}...\")\n",
    "    print(f\"情緒標籤: {train_df['emotion'].iloc[i]}\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "# 檢查預處理後的統計資訊\n",
    "print(f\"\\n訓練集清理後的空文本數量: {(train_df['cleaned_text'] == '').sum()}\")\n",
    "print(f\"測試集清理後的空文本數量: {(test_df['cleaned_text'] == '').sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF 訓練集特徵維度: (47890, 5000)\n",
      "TF-IDF 測試集特徵維度: (16281, 5000)\n",
      "\n",
      "標籤對應關係:\n",
      "  0: anger\n",
      "  1: disgust\n",
      "  2: fear\n",
      "  3: joy\n",
      "  4: sadness\n",
      "  5: surprise\n",
      "\n",
      "額外特徵數量: 7\n",
      "額外特徵欄位: ['text_length', 'cleaned_length', 'word_count', 'avg_word_length', 'exclamation_count', 'question_count', 'caps_ratio']\n",
      "\n",
      "合併後訓練集特徵維度: (47890, 5007)\n",
      "合併後測試集特徵維度: (16281, 5007)\n",
      "\n",
      "前20個 TF-IDF 特徵詞: ['abandoned' 'ability' 'able' 'able get' 'abortion' 'abroad' 'absolute'\n",
      " 'absolutely' 'absolutely love' 'absurd' 'abuse' 'abused' 'abusive'\n",
      " 'accent' 'accept' 'acceptable' 'accepted' 'accepting' 'access' 'accident']\n"
     ]
    }
   ],
   "source": [
    "### Add the code related to the feature engineering steps in cells inside this section\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# 特徵提取，建立 TF-IDF 向量化器\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,      # 最多保留 5000 個特徵詞\n",
    "    ngram_range=(1, 2),     # 使用 unigram 和 bigram\n",
    "    min_df=2,               # 最少出現在 2 個文檔中\n",
    "    max_df=0.95,            # 最多出現在 95% 的文檔中\n",
    "    sublinear_tf=True       # 使用 sublinear TF scaling\n",
    ")\n",
    "\n",
    "# 對訓練集擬合並轉換\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['cleaned_text'])\n",
    "# 對測試集轉換\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_df['cleaned_text'])\n",
    "\n",
    "print(f\"TF-IDF 訓練集特徵維度: {X_train_tfidf.shape}\")\n",
    "print(f\"TF-IDF 測試集特徵維度: {X_test_tfidf.shape}\")\n",
    "\n",
    "# 標籤編碼\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_df['emotion'])\n",
    "print(\"\\n標籤對應關係:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {i}: {label}\")\n",
    "\n",
    "# 額外文本特徵\n",
    "def extract_text_features(df, text_col='text', cleaned_col='cleaned_text'):\n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    # 原始文本長度\n",
    "    features['text_length'] = df[text_col].apply(lambda x: len(str(x)))\n",
    "    \n",
    "    # 清理後文本長度\n",
    "    features['cleaned_length'] = df[cleaned_col].apply(lambda x: len(str(x)))\n",
    "    \n",
    "    # 單詞數量\n",
    "    features['word_count'] = df[cleaned_col].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    # 平均單詞長度\n",
    "    features['avg_word_length'] = df[cleaned_col].apply(\n",
    "        lambda x: np.mean([len(word) for word in str(x).split()]) if len(str(x).split()) > 0 else 0\n",
    "    )\n",
    "    \n",
    "    # 驚嘆號數量（情緒指標）\n",
    "    features['exclamation_count'] = df[text_col].apply(lambda x: str(x).count('!'))\n",
    "    \n",
    "    # 問號數量\n",
    "    features['question_count'] = df[text_col].apply(lambda x: str(x).count('?'))\n",
    "    \n",
    "    # 大寫字母比例（情緒強度指標）\n",
    "    features['caps_ratio'] = df[text_col].apply(\n",
    "        lambda x: sum(1 for c in str(x) if c.isupper()) / max(len(str(x)), 1)\n",
    "    )\n",
    "    \n",
    "    return features\n",
    "\n",
    "# 提取額外特徵\n",
    "train_extra_features = extract_text_features(train_df)\n",
    "test_extra_features = extract_text_features(test_df)\n",
    "\n",
    "print(f\"\\n額外特徵數量: {train_extra_features.shape[1]}\")\n",
    "print(f\"額外特徵欄位: {list(train_extra_features.columns)}\")\n",
    "\n",
    "# 合併所有特徵\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# 將額外特徵轉換為稀疏矩陣並與 TF-IDF 合併\n",
    "X_train_combined = hstack([X_train_tfidf, train_extra_features.values])\n",
    "X_test_combined = hstack([X_test_tfidf, test_extra_features.values])\n",
    "\n",
    "print(f\"\\n合併後訓練集特徵維度: {X_train_combined.shape}\")\n",
    "print(f\"合併後測試集特徵維度: {X_test_combined.shape}\")\n",
    "\n",
    "# 檢視特徵重要性（Top TF-IDF 詞彙）\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(f\"\\n前20個 TF-IDF 特徵詞: {feature_names[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練子集大小: 38312\n",
      "驗證集大小: 9578\n",
      "\n",
      "==================================================\n",
      "模型比較（使用驗證集）\n",
      "==================================================\n",
      "\n",
      "訓練 Multinomial Naive Bayes（僅使用 TF-IDF 特徵）...\n",
      "Multinomial Naive Bayes - Macro F1 Score: 0.2390\n",
      "\n",
      "訓練 Logistic Regression...\n",
      "Logistic Regression - Macro F1 Score: 0.2881\n",
      "\n",
      "訓練 Linear SVC...\n",
      "Linear SVC - Macro F1 Score: 0.4207\n",
      "\n",
      "訓練 Random Forest...\n",
      "Random Forest - Macro F1 Score: 0.3570\n",
      "\n",
      "訓練 Gradient Boosting...\n",
      "Gradient Boosting - Macro F1 Score: 0.3571\n",
      "\n",
      "最佳模型: Linear SVC (F1: 0.4207)\n",
      "\n",
      "==================================================\n",
      "最佳模型詳細評估: Linear SVC\n",
      "==================================================\n",
      "\n",
      "分類報告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.53      0.51      0.52      2139\n",
      "     disgust       0.23      0.05      0.08       237\n",
      "        fear       0.48      0.33      0.39       402\n",
      "         joy       0.68      0.84      0.75      4759\n",
      "     sadness       0.47      0.27      0.35       785\n",
      "    surprise       0.52      0.37      0.43      1256\n",
      "\n",
      "    accuracy                           0.62      9578\n",
      "   macro avg       0.49      0.40      0.42      9578\n",
      "weighted avg       0.59      0.62      0.59      9578\n",
      "\n",
      "\n",
      "使用全部訓練資料重新訓練最佳模型...\n",
      "模型訓練完成！\n",
      "\n",
      "對測試集進行預測...\n",
      "預測完成！共 16281 筆預測\n",
      "預測類別分布:\n",
      "  anger: 3762\n",
      "  disgust: 95\n",
      "  fear: 628\n",
      "  joy: 9463\n",
      "  sadness: 788\n",
      "  surprise: 1545\n",
      "\n",
      "提交檔案已儲存為 'submission.csv'\n",
      "         id  emotion\n",
      "0  0x61fc95     fear\n",
      "1  0xaba820      joy\n",
      "2  0x66e44d      joy\n",
      "3  0xc03cf5      joy\n",
      "4  0x02f65a      joy\n",
      "5  0x479407      joy\n",
      "6  0xe07a21     fear\n",
      "7  0x06d186      joy\n",
      "8  0xa9a658      joy\n",
      "9  0x0a0102  sadness\n"
     ]
    }
   ],
   "source": [
    "### Add the code related to the model implementation steps in cells inside this section\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 資料分割（驗證用）\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train_combined, y_train, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_train\n",
    ")\n",
    "print(f\"訓練子集大小: {X_tr.shape[0]}\")\n",
    "print(f\"驗證集大小: {X_val.shape[0]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"模型比較（使用驗證集）\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 先用 TF-IDF 特徵測試 MultinomialNB\n",
    "print(\"\\n訓練 Multinomial Naive Bayes（僅使用 TF-IDF 特徵）...\")\n",
    "X_tr_tfidf, X_val_tfidf, _, _ = train_test_split(\n",
    "    X_train_tfidf, y_train, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "mnb_model = MultinomialNB(alpha=1.0)  # alpha 是平滑參數\n",
    "mnb_model.fit(X_tr_tfidf, y_tr)\n",
    "y_pred_mnb = mnb_model.predict(X_val_tfidf)\n",
    "f1_mnb = f1_score(y_val, y_pred_mnb, average='macro')\n",
    "print(f\"Multinomial Naive Bayes - Macro F1 Score: {f1_mnb:.4f}\")\n",
    "\n",
    "# 其他模型使用合併特徵\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, C=1.0, random_state=42),\n",
    "    'Linear SVC': LinearSVC(max_iter=2000, C=1.0, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# 初始化最佳模型追蹤（包含 MNB 結果）\n",
    "best_model = mnb_model\n",
    "best_score = f1_mnb\n",
    "best_model_name = \"Multinomial Naive Bayes\"\n",
    "use_tfidf_only = True  # 標記是否只用 TF-IDF\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n訓練 {name}...\")\n",
    "    model.fit(X_tr, y_tr)\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # 計算 Macro F1 Score\n",
    "    f1 = f1_score(y_val, y_pred, average='macro')\n",
    "    print(f\"{name} - Macro F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    if f1 > best_score:\n",
    "        best_score = f1\n",
    "        best_model = model\n",
    "        best_model_name = name\n",
    "        use_tfidf_only = False\n",
    "\n",
    "print(f\"\\n最佳模型: {best_model_name} (F1: {best_score:.4f})\")\n",
    "\n",
    "# 最佳模型詳細評估\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"最佳模型詳細評估: {best_model_name}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if use_tfidf_only:\n",
    "    y_val_pred = best_model.predict(X_val_tfidf)\n",
    "else:\n",
    "    y_val_pred = best_model.predict(X_val)\n",
    "print(\"\\n分類報告:\")\n",
    "print(classification_report(y_val, y_val_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# 使用全部訓練資料重新訓練最佳模型\n",
    "print(\"\\n使用全部訓練資料重新訓練最佳模型...\")\n",
    "\n",
    "if best_model_name == 'Multinomial Naive Bayes':\n",
    "    final_model = MultinomialNB(alpha=1.0)\n",
    "    final_model.fit(X_train_tfidf, y_train)\n",
    "    X_test_final = X_test_tfidf\n",
    "elif best_model_name == 'Logistic Regression':\n",
    "    final_model = LogisticRegression(max_iter=1000, C=1.0, random_state=42)\n",
    "    final_model.fit(X_train_combined, y_train)\n",
    "    X_test_final = X_test_combined\n",
    "elif best_model_name == 'Linear SVC':\n",
    "    final_model = LinearSVC(max_iter=2000, C=1.0, random_state=42)\n",
    "    final_model.fit(X_train_combined, y_train)\n",
    "    X_test_final = X_test_combined\n",
    "elif best_model_name == 'Random Forest':\n",
    "    final_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    final_model.fit(X_train_combined, y_train)\n",
    "    X_test_final = X_test_combined\n",
    "else:\n",
    "    final_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "    final_model.fit(X_train_combined, y_train)\n",
    "    X_test_final = X_test_combined\n",
    "\n",
    "print(\"模型訓練完成！\")\n",
    "\n",
    "# 對測試集進行預測\n",
    "print(\"\\n對測試集進行預測...\")\n",
    "y_test_pred = final_model.predict(X_test_final)\n",
    "\n",
    "# 將預測結果轉換回標籤名稱\n",
    "y_test_labels = label_encoder.inverse_transform(y_test_pred)\n",
    "\n",
    "print(f\"預測完成！共 {len(y_test_labels)} 筆預測\")\n",
    "print(f\"預測類別分布:\")\n",
    "for emotion in label_encoder.classes_:\n",
    "    count = sum(y_test_labels == emotion)\n",
    "    print(f\"  {emotion}: {count}\")\n",
    "\n",
    "# 生成提交檔案\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'].values,\n",
    "    'emotion': y_test_labels\n",
    "})\n",
    "\n",
    "# 儲存提交檔案\n",
    "submission.to_csv('dm-lab-2-private-competition/submission.csv', index=False)\n",
    "print(\"\\n提交檔案已儲存為 'submission.csv'\")\n",
    "print(submission.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dm2025-lab2-exercise (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
